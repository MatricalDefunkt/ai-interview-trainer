{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c8dcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Organized\\Projects\\interview-trainer-ai-model\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from moviepy import VideoFileClip\n",
    "import whisper # OpenAI's Whisper for STT\n",
    "import librosa # For audio analysis\n",
    "import cv2 # OpenCV for video processing\n",
    "from deepface import DeepFace # For facial emotion analysis\n",
    "import mediapipe as mp # For face mesh and pose estimation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdc2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "OUTPUT_AUDIO_FILENAME = \"temp_audio.wav\"\n",
    "\n",
    "def extract_audio(video_path, audio_output_path):\n",
    "    \"\"\"Extracts audio from video file and saves as WAV.\"\"\"\n",
    "    print(f\"Extracting audio from {video_path}...\")\n",
    "    try:\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        audio_clip = video_clip.audio\n",
    "        if audio_clip is None:\n",
    "             print(f\"Error: No audio track found in {video_path}\")\n",
    "             return False\n",
    "        audio_clip.write_audiofile(audio_output_path, codec='pcm_s16le') # Standard WAV codec\n",
    "        audio_clip.close()\n",
    "        video_clip.close()\n",
    "        print(f\"Audio extracted successfully to {audio_output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting audio: {e}\")\n",
    "        # Clean up if partial file exists\n",
    "        if os.path.exists(audio_output_path):\n",
    "            os.remove(audio_output_path)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c421cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_MODEL = \"base\" # Options: \"tiny\", \"base\", \"small\", \"medium\", \"large\". Larger = more accurate but slower/more resource intensive.\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"\n",
    "    Transcribes audio using Whisper, attempting to retain filler words\n",
    "    by requesting word-level timestamps and reconstructing the transcript.\n",
    "    \"\"\"\n",
    "    print(f\"Loading Whisper model ({WHISPER_MODEL})...\")\n",
    "    try:\n",
    "        # Load the model (consider doing this once outside the function if calling repeatedly)\n",
    "        model = whisper.load_model(WHISPER_MODEL)\n",
    "        print(f\"Transcribing audio file: {audio_path} with word timestamps (this may take a while)...\")\n",
    "\n",
    "        # Key change: Set word_timestamps=True\n",
    "        start_time = time.time()\n",
    "        result = model.transcribe(audio_path, word_timestamps=True, fp16=False) # fp16=False might improve stability/accuracy on some systems\n",
    "        end_time = time.time()\n",
    "        print(f\"Transcription complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "        # Reconstruct the transcript from word segments\n",
    "        # This ensures we capture words that might be filtered in the basic 'text' output\n",
    "        full_transcript = \"\"\n",
    "        if 'segments' in result:\n",
    "            all_words = []\n",
    "            for segment in result['segments']:\n",
    "                if 'words' in segment:\n",
    "                    for word_info in segment['words']:\n",
    "                        # word_info is a dict like {'word': ' Hello', 'start': 0.0, 'end': 0.5, 'probability': 0.9}\n",
    "                        # Note: Whisper often includes leading/trailing spaces in word_info['word']\n",
    "                        all_words.append(word_info['word'])\n",
    "\n",
    "            # Join the words carefully. Using strip() on each word and joining with a single space\n",
    "            # handles cases where Whisper includes spaces and avoids double spacing.\n",
    "            full_transcript = \" \".join(word.strip() for word in all_words).strip()\n",
    "\n",
    "            # Alternative simpler join (might have occasional extra spaces if whisper includes them):\n",
    "            # full_transcript = \"\".join(word_info['word'] for segment in result['segments'] if 'words' in segment for word_info in segment['words']).strip()\n",
    "\n",
    "        else:\n",
    "            # Fallback if segments/words aren't available (shouldn't happen with word_timestamps=True)\n",
    "            print(\"Warning: Word segments not found in Whisper result. Falling back to basic text.\")\n",
    "            full_transcript = result.get('text', \"\") # Use basic text if structure is unexpected\n",
    "\n",
    "        if not full_transcript:\n",
    "             print(\"Warning: Transcription resulted in empty text.\")\n",
    "             return \"\" # Return empty string instead of None for consistency downstream\n",
    "\n",
    "        return full_transcript\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription with word timestamps: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for debugging\n",
    "        return None # Return None on error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d616acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio_features(audio_path, transcript):\n",
    "    \"\"\"Analyzes audio features like pace, pauses, pitch, volume.\"\"\"\n",
    "    print(\"Analyzing audio features...\")\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=None) # Load audio with its original sample rate\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        analysis_results = {}\n",
    "\n",
    "        # 1. Pace (Words Per Minute)\n",
    "        word_count = len(transcript.split())\n",
    "        if duration > 0:\n",
    "            wpm = int((word_count / duration) * 60)\n",
    "            analysis_results['pace_wpm'] = wpm\n",
    "            print(f\"- Pace: {wpm} WPM\")\n",
    "        else:\n",
    "             analysis_results['pace_wpm'] = 0\n",
    "             print(\"- Pace: N/A (duration is zero)\")\n",
    "\n",
    "\n",
    "        # 2. Filler Words (Simple Count)\n",
    "        # More sophisticated filler word detection is complex and often requires specific acoustic models\n",
    "        fillers = [\"um\", \"uh\", \"like\", \"you know\", \"so\", \"actually\", \"basically\"]\n",
    "        filler_count = sum(transcript.lower().count(f) for f in fillers)\n",
    "        analysis_results['filler_count'] = filler_count\n",
    "        print(f\"- Filler Words Count (basic): {filler_count}\")\n",
    "\n",
    "        # 3. Volume Analysis (RMS Energy)\n",
    "        rms = librosa.feature.rms(y=y)[0]\n",
    "        avg_volume = np.mean(rms)\n",
    "        std_volume = np.std(rms)\n",
    "        analysis_results['avg_volume_rms'] = float(avg_volume)\n",
    "        analysis_results['std_volume_rms'] = float(std_volume)\n",
    "        print(f\"- Average Volume (RMS): {avg_volume:.4f}\")\n",
    "        print(f\"- Volume Variation (Std Dev RMS): {std_volume:.4f}\")\n",
    "\n",
    "\n",
    "        # 4. Pitch Analysis (Fundamental Frequency - F0)\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "        valid_f0 = f0[voiced_flag] # Consider only voiced segments for pitch stats\n",
    "        if len(valid_f0) > 0:\n",
    "            avg_pitch = np.mean(valid_f0)\n",
    "            std_pitch = np.std(valid_f0)\n",
    "            analysis_results['avg_pitch_hz'] = float(avg_pitch)\n",
    "            analysis_results['std_pitch_hz'] = float(std_pitch)\n",
    "            print(f\"- Average Pitch (F0): {avg_pitch:.2f} Hz\")\n",
    "            print(f\"- Pitch Variation (Std Dev F0): {std_pitch:.2f} Hz\")\n",
    "        else:\n",
    "            analysis_results['avg_pitch_hz'] = 0\n",
    "            analysis_results['std_pitch_hz'] = 0\n",
    "            print(\"- Pitch: Could not reliably detect pitch.\")\n",
    "\n",
    "\n",
    "        # 5. Pause Analysis (Simple Silence Detection)\n",
    "        # Use librosa's split based on RMS energy threshold\n",
    "        # top_db=40 means consider anything 40dB below the max RMS as silence\n",
    "        non_silent_intervals = librosa.effects.split(y, top_db=40)\n",
    "        pauses = []\n",
    "        last_end = 0\n",
    "        for start, end in non_silent_intervals:\n",
    "            pause_duration = (start / sr) - (last_end / sr)\n",
    "            if pause_duration > 0.2: # Consider pauses longer than 200ms\n",
    "                pauses.append(pause_duration)\n",
    "            last_end = end\n",
    "        # Check pause after last segment until end of audio\n",
    "        final_pause = duration - (last_end / sr)\n",
    "        if final_pause > 0.2:\n",
    "             pauses.append(final_pause)\n",
    "\n",
    "        analysis_results['num_pauses'] = len(pauses)\n",
    "        analysis_results['avg_pause_duration_s'] = float(np.mean(pauses)) if pauses else 0\n",
    "        print(f\"- Number of Pauses (>0.2s): {len(pauses)}\")\n",
    "        if pauses:\n",
    "            print(f\"- Average Pause Duration: {np.mean(pauses):.2f} s\")\n",
    "\n",
    "        return analysis_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during audio feature analysis: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85609df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f5512a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom emotion model architecture created and weights loaded successfully from best_model_full.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # Add this import\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense # Add these layer imports\n",
    "from tensorflow.keras.optimizers import Adam # Add optimizer import (needed for compile)\n",
    "\n",
    "# --- Define Model Architecture ---\n",
    "IMG_SIZE = (48, 48) # Make sure this matches training\n",
    "NUM_CLASSES = 7     # Make sure this matches training\n",
    "\n",
    "def create_model_simple():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # NOTE: Define input_shape *without* the batch dimension here\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax') # Use NUM_CLASSES\n",
    "    ])\n",
    "    # Compile is necessary after loading weights for the model to be usable for prediction\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Model Loading and Configuration ---\n",
    "model_path = 'best_model_full.h5'  # Make sure this path is correct\n",
    "loaded_emotion_model = None # Initialize as None\n",
    "\n",
    "try:\n",
    "    # 1. Create the model architecture\n",
    "    loaded_emotion_model = create_model_simple()\n",
    "    # 2. Load only the weights\n",
    "    loaded_emotion_model.load_weights(model_path)\n",
    "    print(f\"Custom emotion model architecture created and weights loaded successfully from {model_path}\")\n",
    "    # loaded_emotion_model.summary() # Optional: check summary\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model architecture or loading weights: {e}\")\n",
    "    loaded_emotion_model = None # Ensure it's None if loading fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71eb6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Define Class Labels (Ensure this order matches training) ---\n",
    "# Based on alphabetical sorting typically used by ImageDataGenerator:\n",
    "class_labels = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "target_size = (48, 48) # The input size your model expects\n",
    "\n",
    "# --- Constants and MediaPipe Initialization (Keep as is) ---\n",
    "VISUALIZE = True\n",
    "WINDOW_NAME = 'Interview Analysis Visualization'\n",
    "VIDEO_ANALYSIS_FRAME_SKIP = 2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_pose = mp.solutions.pose\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5, refine_landmarks=True, min_tracking_confidence=0.5)\n",
    "pose_estimator = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def analyze_video_features(video_path):\n",
    "    \"\"\"\n",
    "    Analyzes video for facial expressions (using custom Keras model), eye contact, posture,\n",
    "    and provides visual feedback if VISUALIZE is True.\n",
    "    \"\"\"\n",
    "    if loaded_emotion_model is None:\n",
    "        print(\"Emotion model not loaded. Skipping emotion analysis.\")\n",
    "        # Handle how you want the function to behave if the model isn't loaded\n",
    "        # Maybe return partial results or None\n",
    "\n",
    "    print(\"Analyzing video features (using custom model)...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return None\n",
    "\n",
    "    frame_count = 0\n",
    "    processed_frame_count = 0\n",
    "    # Initialize emotion counts dictionary using the defined labels\n",
    "    emotion_counts = {label: 0 for label in class_labels}\n",
    "    eye_contact_frames = 0\n",
    "    upright_frames = 0\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    if VISUALIZE:\n",
    "        cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # End of video\n",
    "\n",
    "        frame_count += 1\n",
    "        if frame_count % VIDEO_ANALYSIS_FRAME_SKIP != 0:\n",
    "            continue # Skip frame\n",
    "\n",
    "        processed_frame_count += 1\n",
    "        start_time_frame = time.time()\n",
    "\n",
    "        annotated_frame = frame.copy()\n",
    "        rgb_frame_mp = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Use RGB for MediaPipe\n",
    "        rgb_frame_mp.flags.writeable = False\n",
    "\n",
    "        # --- MediaPipe Face Mesh and Pose Processing ---\n",
    "        face_results = face_mesh.process(rgb_frame_mp)\n",
    "        pose_results = pose_estimator.process(rgb_frame_mp)\n",
    "\n",
    "        rgb_frame_mp.flags.writeable = True # Re-enable if needed later\n",
    "\n",
    "        # --- Facial Emotion Analysis (Using Custom Keras Model) ---\n",
    "        current_emotion = \"N/A\" # Default value\n",
    "\n",
    "        if loaded_emotion_model: # Check if the model was loaded successfully\n",
    "            try:\n",
    "                # 1. Preprocess the *current frame* for the emotion model\n",
    "                # Convert frame to RGB (if not already done for MP)\n",
    "                rgb_frame_emotion = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # Resize to the target size (48x48)\n",
    "                img_resized = cv2.resize(rgb_frame_emotion, target_size)\n",
    "                # Convert to float and rescale pixel values to [0, 1]\n",
    "                img_array = np.array(img_resized, dtype=np.float32) / 255.0\n",
    "                # Add the batch dimension (shape becomes 1, 48, 48, 3)\n",
    "                img_batched = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "                # 2. Predict using the loaded Keras model\n",
    "                # Use verbose=0 to avoid printing progress for every frame\n",
    "                predictions = loaded_emotion_model.predict(img_batched, verbose=0)\n",
    "\n",
    "                # 3. Interpret the prediction\n",
    "                predicted_index = np.argmax(predictions[0]) # Get index of max probability\n",
    "                current_emotion = class_labels[predicted_index] # Map index to label\n",
    "\n",
    "                # Update counts\n",
    "                emotion_counts[current_emotion] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch potential errors during preprocessing or prediction\n",
    "                # print(f\"Frame {frame_count}: Custom Emotion Model error: {e}\") # Optional for debugging\n",
    "                current_emotion = \"Error\" # Indicate an error occurred for this frame\n",
    "                pass # Continue processing the video\n",
    "        is_eye_contact = False # Flag for current frame\n",
    "        if face_results.multi_face_landmarks:\n",
    "            for face_landmarks in face_results.multi_face_landmarks:\n",
    "                # Draw face mesh\n",
    "                if VISUALIZE:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=annotated_frame,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=annotated_frame,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                        landmark_drawing_spec=None,\n",
    "                        connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "                    # Iris landmarks drawing (optional, can be busy)\n",
    "                    # mp_drawing.draw_landmarks(\n",
    "                    #     image=annotated_frame,\n",
    "                    #     landmark_list=face_landmarks,\n",
    "                    #     connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                    #     landmark_drawing_spec=None,\n",
    "                    #     connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "                # Calculate eye contact\n",
    "                try:\n",
    "                    left_pupil = face_landmarks.landmark[473] # Corrected index? Check docs if unsure. Typically 473-477 are right iris, 468-472 left. Let's use documented iris centers.\n",
    "                    right_pupil = face_landmarks.landmark[468] # Corrected index? Let's use documented iris centers.\n",
    "\n",
    "                    # Get corners for width calculation\n",
    "                    left_eye_inner = face_landmarks.landmark[133]\n",
    "                    left_eye_outer = face_landmarks.landmark[33]\n",
    "                    right_eye_inner = face_landmarks.landmark[362]\n",
    "                    right_eye_outer = face_landmarks.landmark[263]\n",
    "\n",
    "                    left_eye_width = abs(left_eye_outer.x - left_eye_inner.x)\n",
    "                    right_eye_width = abs(right_eye_outer.x - right_eye_inner.x)\n",
    "\n",
    "                    if left_eye_width > 0.01 and right_eye_width > 0.01: # Avoid division by zero\n",
    "                        # Use the correct pupil landmark index for relative position calculation\n",
    "                        left_pupil_rel_pos = (face_landmarks.landmark[468].x - left_eye_inner.x) / left_eye_width # Use left pupil center [468]\n",
    "                        right_pupil_rel_pos = (face_landmarks.landmark[473].x - right_eye_inner.x) / right_eye_width # Use right pupil center [473]\n",
    "\n",
    "                        # Thresholds for looking 'forward' (TUNING NEEDED!)\n",
    "                        if 0.3 < left_pupil_rel_pos < 0.7 and 0.3 < right_pupil_rel_pos < 0.7:\n",
    "                            is_eye_contact = True\n",
    "                            eye_contact_frames += 1\n",
    "                            # print(\"Eye contact DETECTED\") # Debug print\n",
    "\n",
    "                    # Visualize Eye Contact state (draw pupils differently)\n",
    "                    if VISUALIZE:\n",
    "                        pupil_color = (0, 255, 0) if is_eye_contact else (0, 0, 255) # Green if contact, Red if not\n",
    "                        # Get pixel coordinates\n",
    "                        l_pupil_px = mp_drawing._normalized_to_pixel_coordinates(face_landmarks.landmark[468].x, face_landmarks.landmark[468].y, frame_width, frame_height)\n",
    "                        r_pupil_px = mp_drawing._normalized_to_pixel_coordinates(face_landmarks.landmark[473].x, face_landmarks.landmark[473].y, frame_width, frame_height)\n",
    "                        if l_pupil_px and r_pupil_px:\n",
    "                            cv2.circle(annotated_frame, l_pupil_px, 3, pupil_color, -1)\n",
    "                            cv2.circle(annotated_frame, r_pupil_px, 3, pupil_color, -1)\n",
    "\n",
    "                    break # Process only the first detected face\n",
    "\n",
    "                except IndexError:\n",
    "                     print(f\"Warning: Iris landmarks (468, 473) not found. Ensure 'refine_landmarks=True' is set.\")\n",
    "                     # Draw basic mesh even if iris fails\n",
    "                     if VISUALIZE:\n",
    "                         mp_drawing.draw_landmarks(\n",
    "                            image=annotated_frame,\n",
    "                            landmark_list=face_landmarks,\n",
    "                            connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "                         mp_drawing.draw_landmarks(\n",
    "                            image=annotated_frame,\n",
    "                            landmark_list=face_landmarks,\n",
    "                            connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "                except Exception as e_eye:\n",
    "                    print(f\"Error during eye contact calculation/drawing: {e_eye}\")\n",
    "\n",
    "\n",
    "        # --- Posture Heuristic Calculation & Visualization ---\n",
    "        is_upright = False # Flag for current frame\n",
    "        if pose_results.pose_landmarks:\n",
    "             # Draw the pose skeleton\n",
    "            if VISUALIZE:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    annotated_frame,\n",
    "                    pose_results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "            # Calculate posture\n",
    "            landmarks = pose_results.pose_landmarks.landmark\n",
    "            left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "            right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "            if left_shoulder.visibility > 0.6 and right_shoulder.visibility > 0.6:\n",
    "                y_diff = abs(left_shoulder.y - right_shoulder.y) * frame_height # Pixel diff\n",
    "                if y_diff < frame_height * 0.1: # Shoulders relatively level\n",
    "                    is_upright = True\n",
    "                    upright_frames += 1\n",
    "\n",
    "            # Visualize Posture state (draw shoulder line differently)\n",
    "            if VISUALIZE:\n",
    "                posture_color = (0, 255, 0) if is_upright else (0, 0, 255) # Green if upright, Red if not\n",
    "                ls_px = mp_drawing._normalized_to_pixel_coordinates(left_shoulder.x, left_shoulder.y, frame_width, frame_height)\n",
    "                rs_px = mp_drawing._normalized_to_pixel_coordinates(right_shoulder.x, right_shoulder.y, frame_width, frame_height)\n",
    "                if ls_px and rs_px and left_shoulder.visibility > 0.6 and right_shoulder.visibility > 0.6:\n",
    "                    cv2.line(annotated_frame, ls_px, rs_px, posture_color, 2)\n",
    "\n",
    "\n",
    "        # --- Display Text Info on Frame ---\n",
    "        if VISUALIZE:\n",
    "            y_pos = 30 # Starting Y position for text\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.7\n",
    "            text_color = (255, 255, 255) # White\n",
    "            bg_color = (0, 0, 0) # Black background for text\n",
    "            thickness = 2\n",
    "\n",
    "            # Emotion Text\n",
    "            text_emotion = f\"Emotion: {current_emotion}\"\n",
    "            (w, h), _ = cv2.getTextSize(text_emotion, font, font_scale, thickness)\n",
    "            cv2.rectangle(annotated_frame, (10, y_pos - h - 5) , (10 + w + 5, y_pos + 5), bg_color, -1)\n",
    "            cv2.putText(annotated_frame, text_emotion, (10, y_pos), font, font_scale, text_color, thickness)\n",
    "            y_pos += h + 10\n",
    "\n",
    "            # Eye Contact Text\n",
    "            text_eye = f\"Eye Contact: {'YES' if is_eye_contact else 'NO'}\"\n",
    "            (w, h), _ = cv2.getTextSize(text_eye, font, font_scale, thickness)\n",
    "            cv2.rectangle(annotated_frame, (10, y_pos - h - 5) , (10 + w + 5, y_pos + 5), bg_color, -1)\n",
    "            cv2.putText(annotated_frame, text_eye, (10, y_pos), font, font_scale, (0, 255, 0) if is_eye_contact else (0, 0, 255), thickness)\n",
    "            y_pos += h + 10\n",
    "\n",
    "            # Posture Text\n",
    "            text_posture = f\"Posture: {'Upright' if is_upright else 'Not Upright'}\"\n",
    "            (w, h), _ = cv2.getTextSize(text_posture, font, font_scale, thickness)\n",
    "            cv2.rectangle(annotated_frame, (10, y_pos - h - 5) , (10 + w + 5, y_pos + 5), bg_color, -1)\n",
    "            cv2.putText(annotated_frame, text_posture, (10, y_pos), font, font_scale, (0, 255, 0) if is_upright else (0, 0, 255), thickness)\n",
    "\n",
    "        # --- Display Frame ---\n",
    "        if VISUALIZE:\n",
    "            end_time_frame = time.time()\n",
    "            processing_time = end_time_frame - start_time_frame\n",
    "            fps = 1.0 / processing_time if processing_time > 0 else 0\n",
    "            cv2.putText(annotated_frame, f\"FPS: {fps:.1f}\", (frame_width - 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "\n",
    "            cv2.imshow(WINDOW_NAME, annotated_frame)\n",
    "            # Allow window events and check for 'q' key to quit\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                print(\"Visualization stopped by user ('q' pressed).\")\n",
    "                break\n",
    "\n",
    "        # Progress print to console\n",
    "        if processed_frame_count % 20 == 0: # Print progress less often now\n",
    "             print(f\"...processed video frame {frame_count} (Total processed: {processed_frame_count})\")\n",
    "\n",
    "    # --- Cleanup and Final Calculations ---\n",
    "    cap.release()\n",
    "    if VISUALIZE:\n",
    "        cv2.destroyAllWindows()\n",
    "    print(\"Video analysis complete.\")\n",
    "\n",
    "    # Compile results (same as before)\n",
    "    video_analysis_results = {}\n",
    "    if processed_frame_count > 0:\n",
    "        dominant_emotion = max(emotion_counts, key=emotion_counts.get) if emotion_counts else \"N/A\"\n",
    "        video_analysis_results['dominant_emotion'] = dominant_emotion\n",
    "        video_analysis_results['emotion_distribution'] = emotion_counts\n",
    "        # video_analysis_results['eye_contact_percentage'] = round((eye_contact_frames / processed_frame_count) * 100, 2)\n",
    "        video_analysis_results['upright_posture_percentage'] = round((upright_frames / processed_frame_count) * 100, 2)\n",
    "\n",
    "        print(f\"- Dominant Emotion Detected: {dominant_emotion}\")\n",
    "        print(f\"- Emotion Distribution: {emotion_counts}\")\n",
    "        # print(f\"- Estimated Eye Contact: {video_analysis_results['eye_contact_percentage']}%\")\n",
    "        print(f\"- Estimated Upright Posture: {video_analysis_results['upright_posture_percentage']}%\")\n",
    "    else:\n",
    "        print(\"- No frames processed for video analysis.\")\n",
    "        video_analysis_results['error'] = \"No frames processed\"\n",
    "\n",
    "    return video_analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e882123",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'video_file': 'confident_interview.mp4', 'question': 'Tell me about yourself'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Starting Interview Analysis\n",
      "Video File: Tell me about yourself\n",
      "Interview Question: nervous_interview.mp4\n",
      "------------------------------\n",
      "Extracting audio from nervous_interview.mp4...\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'mp42isom'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [474, 850], 'bitrate': 1578, 'fps': 30.1, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 48000, 'bitrate': 156, 'metadata': {'Metadata': '', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 18.13, 'bitrate': 1736, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [474, 850], 'video_bitrate': 1578, 'video_fps': 30.1, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 48000, 'audio_bitrate': 156, 'video_duration': 18.13, 'video_n_frames': 545}\n",
      "d:\\Organized\\Projects\\interview-trainer-ai-model\\Lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i nervous_interview.mp4 -loglevel error -f image2pipe -vf scale=474:850 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted successfully to temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "print(\"Starting Interview Analysis\")\n",
    "print(f\"Video File: {args['question']}\")\n",
    "print(f\"Interview Question: {args['video_file']}\")\n",
    "print(\"-\" * 30)\n",
    "# 1. Extract Audio\n",
    "if not extract_audio(args['video_file'], OUTPUT_AUDIO_FILENAME):\n",
    "    print(\"Analysis aborted due to audio extraction failure.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd21450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model (base)...\n",
      "Transcribing audio file: temp_audio.wav with word timestamps (this may take a while)...\n",
      "Transcription complete in 13.39 seconds.\n",
      "\n",
      "--- Transcription ---\n",
      "My name is Priyant Chibhargav and I've done my BTEC in computer science and... What do you mean by that? I've worked on several projects.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Transcribe Audio\n",
    "transcript = transcribe_audio(OUTPUT_AUDIO_FILENAME)\n",
    "if transcript is None:\n",
    "    print(\"Analysis aborted due to transcription failure.\")\n",
    "        # Clean up audio file before exiting\n",
    "    if os.path.exists(OUTPUT_AUDIO_FILENAME):\n",
    "        os.remove(OUTPUT_AUDIO_FILENAME)\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\n--- Transcription ---\")\n",
    "print(transcript)\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e0aac94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Audio Delivery Analysis ---\n",
      "Analyzing audio features...\n",
      "- Pace: 82 WPM\n",
      "- Filler Words Count (basic): 0\n",
      "- Average Volume (RMS): 0.0482\n",
      "- Volume Variation (Std Dev RMS): 0.0420\n",
      "- Average Pitch (F0): 96.61 Hz\n",
      "- Pitch Variation (Std Dev F0): 14.40 Hz\n",
      "- Number of Pauses (>0.2s): 0\n",
      "- Pace Wpm: 82\n",
      "- Filler Count: 0\n",
      "- Avg Volume Rms: 0.048159483820199966\n",
      "- Std Volume Rms: 0.04201605170965195\n",
      "- Avg Pitch Hz: 96.6124966389549\n",
      "- Std Pitch Hz: 14.395560301373827\n",
      "- Num Pauses: 0\n",
      "- Avg Pause Duration S: 0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Analyze Audio Features\n",
    "print(\"\\n--- Audio Delivery Analysis ---\")\n",
    "audio_metrics = analyze_audio_features(OUTPUT_AUDIO_FILENAME, transcript)\n",
    "if audio_metrics is None:\n",
    "    print(\"Could not perform detailed audio analysis.\") \n",
    "elif audio_metrics:\n",
    "    for key, value in audio_metrics.items():\n",
    "        print(f\"- {key.replace('_', ' ').title()}: {value}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "737a4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Video Delivery Analysis ---\n",
      "Analyzing video features (using custom model)...\n",
      "...processed video frame 40 (Total processed: 20)\n",
      "...processed video frame 80 (Total processed: 40)\n",
      "...processed video frame 120 (Total processed: 60)\n",
      "...processed video frame 160 (Total processed: 80)\n",
      "...processed video frame 200 (Total processed: 100)\n",
      "...processed video frame 240 (Total processed: 120)\n",
      "...processed video frame 280 (Total processed: 140)\n",
      "...processed video frame 320 (Total processed: 160)\n",
      "...processed video frame 360 (Total processed: 180)\n",
      "...processed video frame 400 (Total processed: 200)\n",
      "...processed video frame 440 (Total processed: 220)\n",
      "...processed video frame 480 (Total processed: 240)\n",
      "Video analysis complete.\n",
      "- Dominant Emotion Detected: happy\n",
      "- Emotion Distribution: {'angry': 14, 'disgusted': 5, 'fearful': 8, 'happy': 175, 'neutral': 15, 'sad': 0, 'surprised': 35}\n",
      "- Estimated Upright Posture: 100.0%\n",
      "- Dominant Emotion: happy\n",
      "- Emotion Distribution: {'angry': 14, 'disgusted': 5, 'fearful': 8, 'happy': 175, 'neutral': 15, 'sad': 0, 'surprised': 35}\n",
      "- Upright Posture Percentage: 100.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 4. Analyze Video Features\n",
    "print(\"\\n--- Video Delivery Analysis ---\")\n",
    "video_metrics = analyze_video_features(args['video_file'])\n",
    "if video_metrics is None:\n",
    "    print(\"Could not perform video analysis.\")\n",
    "elif video_metrics and 'error' not in video_metrics :\n",
    "    for key, value in video_metrics.items():\n",
    "            print(f\"- {key.replace('_', ' ').title()}: {value}\")\n",
    "elif video_metrics and 'error' in video_metrics:\n",
    "        print(f\"- Error: {video_metrics['error']}\")\n",
    "else:\n",
    "    print(\"N/A\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb082316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temporary audio file temp_audio.wav deleted.\n"
     ]
    }
   ],
   "source": [
    "# 5. Clean up temporary audio file\n",
    "if os.path.exists(OUTPUT_AUDIO_FILENAME):\n",
    "    try:\n",
    "        os.remove(OUTPUT_AUDIO_FILENAME)\n",
    "        print(f\"\\nTemporary audio file {OUTPUT_AUDIO_FILENAME} deleted.\")\n",
    "    except OSError as e:\n",
    "        print(f\"\\nWarning: Could not delete temporary audio file {OUTPUT_AUDIO_FILENAME}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65f8cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analysis Summary ---\n",
      "Question: Tell me about yourself\n",
      "Video File: confident_interview.mp4\n",
      "\n",
      "Transcript:\n",
      " Good evening, I am Priyad Ji Bharkiv and I am a computer science student at Symbhazis Institute of Technology and I am doing my PTEC and I have worked on many projects including the Moonstack projects and AI as well.\n",
      "\n",
      "Audio Metrics:\n",
      "- Pace Wpm: 141\n",
      "- Filler Count: 0\n",
      "- Avg Volume Rms: 0.04377371445298195\n",
      "- Std Volume Rms: 0.027856705710291862\n",
      "- Avg Pitch Hz: 103.13692304414731\n",
      "- Std Pitch Hz: 18.258189527073117\n",
      "- Num Pauses: 0\n",
      "- Avg Pause Duration S: 0\n",
      "\n",
      "Video Metrics:\n",
      "- Dominant Emotion: N/A\n",
      "- Emotion Distribution: {}\n",
      "- Eye Contact Percentage: 0.0\n",
      "- Upright Posture Percentage: 100.0\n",
      "------------------------------\n",
      "Analysis Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Analysis Summary ---\")\n",
    "print(f\"Question: {args['question']}\")\n",
    "print(f\"Video File: {args['video_file']}\")\n",
    "print(\"\\nTranscript:\")\n",
    "print(transcript if transcript else \"N/A\")\n",
    "print(\"\\nAudio Metrics:\")\n",
    "if audio_metrics:\n",
    "    for key, value in audio_metrics.items():\n",
    "        print(f\"- {key.replace('_', ' ').title()}: {value}\")\n",
    "else:\n",
    "    print(\"N/A\")\n",
    "print(\"\\nVideo Metrics:\")\n",
    "if video_metrics and 'error' not in video_metrics :\n",
    "    for key, value in video_metrics.items():\n",
    "            print(f\"- {key.replace('_', ' ').title()}: {value}\")\n",
    "elif video_metrics and 'error' in video_metrics:\n",
    "        print(f\"- Error: {video_metrics['error']}\")\n",
    "else:\n",
    "    print(\"N/A\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Analysis Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert AI Interview Coach. Your task is to analyze a candidate's response to an interview question based on the provided transcript and objective delivery metrics (audio/video analysis).\n",
    "\n",
    "**Interview Context:**\n",
    "*   **Question Asked:** \"{interview_question}\"\n",
    "\n",
    "**Candidate's Response:**\n",
    "*   **Transcript:** \"{transcript}\"\n",
    "\n",
    "**Objective Delivery Analysis Data:**\n",
    "{audio_metrics_str}\n",
    "{video_metrics_str}\n",
    "\n",
    "**Analysis Instructions:**\n",
    "Evaluate the candidate's performance comprehensively based *only* on the provided data. Consider both the *content* of the answer (using the transcript) and the *delivery* (using the provided audio/video metrics).\n",
    "\n",
    "*   **Content & Delivery Integration:** Assess the following aspects, assigning a score from 1 (Poor) to 5 (Excellent) for each:\n",
    "    *   **Relevance:** How directly and effectively does the answer address the specific question asked? (1=Irrelevant, 5=Highly Relevant)\n",
    "    *   **Clarity:** How clear, concise, and easy to understand is the response, considering both the language used and the delivery (e.g., pace, fillers)? (1=Unclear/Rambling, 5=Very Clear/Concise)\n",
    "    *   **Tone:** How appropriate and effective is the perceived tone for an interview? Consider confidence, professionalism, and engagement, inferred from vocal cues (pitch/volume variation), facial expressions (dominant emotion), and language. (1=Inappropriate/Disengaged/Unconfident, 5=Confident/Professional/Engaging)\n",
    "    *   **Vocabulary:** How appropriate, professional, and articulate is the language used? (1=Inappropriate/Informal/Unclear, 5=Highly Professional/Articulate)\n",
    "    *   **STAR Format Adhesion:** If the question is behavioral, how well does the answer adhere to the STAR method (Situation, Task, Action, Result)? Are all components present and distinct? (1=No Adherence/Not Applicable, 3=Partial Adherence, 5=Excellent Adherence - All parts clear). Assign 1 if not a behavioral question or if format is totally absent.\n",
    "\n",
    "**Output Format:**\n",
    "Provide your analysis *strictly* in JSON format. The JSON object should have the following keys ONLY:\n",
    "\n",
    "*   `relevance_score`: (Integer) Score from 1-5.\n",
    "*   `clarity_score`: (Integer) Score from 1-5.\n",
    "*   `tone_score`: (Integer) Score from 1-5 assessing perceived tone's effectiveness.\n",
    "*   `vocabulary_score`: (Integer) Score from 1-5 assessing language use.\n",
    "*   `star_format_score`: (Integer) Score from 1-5 assessing STAR method adhesion (1 if N/A or completely missing).\n",
    "*   `strengths`: (List of strings) 2-3 bullet points highlighting specific positive aspects related to the scored criteria (e.g., \"Strong relevance (Score: 5).\", \"Tone perceived as confident (Score: 4).\").\n",
    "*   `areas_for_improvement`: (List of strings) 2-4 specific, actionable feedback points related to the scored criteria, referencing metrics or transcript parts where possible (e.g., \"Improve STAR adhesion (Score: 2) by explicitly stating the Result.\", \"Reduce filler word count (count: Y) to enhance clarity (Score: 3).\", \"Work on varying vocal pitch (Std Dev: Z Hz) to improve tone perception (Score: 2).\").\n",
    "\n",
    "**Example JSON Structure:**\n",
    "```json\n",
    "{{\n",
    "  \"relevance_score\": 4,\n",
    "  \"clarity_score\": 3,\n",
    "  \"tone_score\": 4,\n",
    "  \"vocabulary_score\": 5,\n",
    "  \"star_format_score\": 3,\n",
    "  \"strengths\": [\n",
    "    \"Excellent vocabulary use, very professional (Score: 5).\",\n",
    "    \"Answer was highly relevant to the question asked (Score: 4).\",\n",
    "    \"Tone came across as generally confident (Score: 4).\"\n",
    "  ],\n",
    "  \"areas_for_improvement\": [\n",
    "    \"Improve clarity (Score: 3) by structuring points more logically and reducing minor rambling.\",\n",
    "    \"STAR format adhesion was partial (Score: 3); ensure the 'Result' is clearly articulated.\",\n",
    "    \"Consider increasing eye contact (estimated Z%) to further enhance engagement aspect of tone.\",\n",
    "    \"Slightly high filler word count (count: Y) impacted clarity.\"\n",
    "  ]\n",
    "}}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview-trainer-ai-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
